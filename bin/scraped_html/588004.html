<!DOCTYPE html><html lang="en"><head><style>@import url("https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200;1,200;1,300&family=Work+Sans:wght@400;500&display=swap");</style><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><head><link href="../../src/css/general.css" rel="stylesheet" /></head><p class='question-hyperlink'>Is floating point math broken?</p><div class="s-prose js-post-body"><p>Consider the following code:</p>
<pre class="lang-js prettyprint-override"><code>0.1 + 0.2 == 0.3  -&gt;  false
</code></pre>
<pre class="lang-js prettyprint-override"><code>0.1 + 0.2         -&gt;  0.30000000000000004
</code></pre>
<p>Why do these inaccuracies happen?</p>
</div><p class="this-has-helped">This answer has helped 2853 people.</p><div class="s-prose js-post-body"><p>Binary <a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format" rel="noreferrer">floating point</a> math is like this. In most programming languages, it is based on the <a href="https://en.wikipedia.org/wiki/IEEE_754#Basic_and_interchange_formats" rel="noreferrer">IEEE 754 standard</a>. The crux of the problem is that numbers are represented in this format as a whole number times a power of two; rational numbers (such as <code>0.1</code>, which is <code>1/10</code>) whose denominator is not a power of two cannot be exactly represented.</p>
<p>For <code>0.1</code> in the standard <code>binary64</code> format, the representation can be written exactly as</p>
<ul>
<li><code>0.1000000000000000055511151231257827021181583404541015625</code> in decimal, or</li>
<li><code>0x1.999999999999ap-4</code> in <a href="http://www.exploringbinary.com/hexadecimal-floating-point-constants/" rel="noreferrer">C99 hexfloat notation</a>.</li>
</ul>
<p>In contrast, the rational number <code>0.1</code>, which is <code>1/10</code>, can be written exactly as</p>
<ul>
<li><code>0.1</code> in decimal, or</li>
<li><code>0x1.99999999999999...p-4</code> in an analogue of C99 hexfloat notation, where the <code>...</code> represents an unending sequence of 9's.</li>
</ul>
<p>The constants <code>0.2</code> and <code>0.3</code> in your program will also be approximations to their true values.  It happens that the closest <code>double</code> to <code>0.2</code> is larger than the rational number <code>0.2</code> but that the closest <code>double</code> to <code>0.3</code> is smaller than the rational number <code>0.3</code>.  The sum of <code>0.1</code> and <code>0.2</code> winds up being larger than the rational number <code>0.3</code> and hence disagreeing with the constant in your code.</p>
<p>A fairly comprehensive treatment of floating-point arithmetic issues is <a href="http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html" rel="noreferrer"><em>What Every Computer Scientist Should Know About Floating-Point Arithmetic</em></a>. For an easier-to-digest explanation, see <a href="http://floating-point-gui.de" rel="noreferrer">floating-point-gui.de</a>.</p>
<blockquote>
<p><strong>Side Note: All positional (base-N) number systems share this problem with precision</strong></p>
</blockquote>
<p>Plain old decimal (base 10) numbers have the same issues, which is why numbers like 1/3 end up as 0.333333333...</p>
<p>You've just stumbled on a number (3/10) that happens to be easy to represent with the decimal system, but doesn't fit the binary system. It goes both ways (to some small degree) as well: 1/16 is an ugly number in decimal (0.0625), but in binary it looks as neat as a 10,000th does in decimal (0.0001)** - if we were in the habit of using a base-2 number system in our daily lives, you'd even look at that number and instinctively understand you could arrive there by halving something, halving it again, and again and again.</p>
<p>Of course, that's not exactly how floating-point numbers are stored in memory (they use a form of scientific notation). However, it does illustrate the point that binary floating-point precision errors tend to crop up because the &quot;real world&quot; numbers we are usually interested in working with are so often powers of ten - but only because we use a decimal number system day-to-day. This is also why we'll say things like 71% instead of &quot;5 out of every 7&quot; (71% is an approximation, since 5/7 can't be represented exactly with any decimal number).</p>
<p>So no: binary floating point numbers are not broken, they just happen to be as imperfect as every other base-N number system :)</p>
<blockquote>
<p><strong>Side Side Note: Working with Floats in Programming</strong></p>
</blockquote>
<p>In practice, this problem of precision means you need to use rounding functions to round your floating point numbers off to however many decimal places you're interested in before you display them.</p>
<p>You also need to replace equality tests with comparisons that allow some amount of tolerance, which means:</p>
<p>Do <strong>not</strong> do <code>if (x == y) { ... }</code></p>
<p>Instead do <code>if (abs(x - y) &lt; myToleranceValue) { ... }</code>.</p>
<p>where <code>abs</code> is the absolute value. <code>myToleranceValue</code> needs to be chosen for your particular application - and it will have a lot to do with how much &quot;wiggle room&quot; you are prepared to allow, and what the largest number you are going to be comparing may be (due to loss of precision issues). Beware of &quot;epsilon&quot; style constants in your language of choice. These <strong>can</strong> be used as tolerance values but their effectiveness depends on the magnitude (size) of the numbers you're working with, since calculations with large numbers may exceed the epsilon threshold.</p>
</div><p class="this-has-helped">This answer has helped 705 people.</p><div class="s-prose js-post-body"><h1><strong>A Hardware Designer's Perspective</strong></h1>

<p>I believe I should add a hardware designerâ€™s perspective to this since I design and build floating point hardware. Knowing the origin of the error may help in understanding what is happening in the software, and ultimately, I hope this helps explain the reasons for why floating point errors happen and seem to accumulate over time.</p>

<h2>1. Overview</h2>

<p>From an engineering perspective, most floating point operations will have some element of error since the hardware that does the floating point computations is only required to have an error of less than one half of one unit in the last place. Therefore, much hardware will stop at a precision that's only necessary to yield an error of less than one half of one unit in the last place for a <em>single operation</em> which is especially problematic in floating point division. What constitutes a single operation depends upon how many operands the unit takes. For most, it is two, but some units take 3 or more operands. Because of this, there is no guarantee that repeated operations will result in a desirable error since the errors add up over time.</p>

<h2>2. Standards</h2>

<p>Most processors follow the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008" rel="noreferrer">IEEE-754</a> standard but some use denormalized, or different standards
. For example, there is a denormalized mode in IEEE-754 which allows representation of very small floating point numbers at the expense of precision. The following, however, will cover the normalized mode of IEEE-754 which is the typical mode of operation.</p>

<p>In the IEEE-754 standard, hardware designers are allowed any value of error/epsilon as long as it's less than one half of one unit in the last place, and the result only has to be less than one half of one unit in the last place for one operation. This explains why when there are repeated operations, the errors add up. For IEEE-754 double precision, this is the 54th bit, since 53 bits are used to represent the numeric part (normalized), also called the mantissa, of the floating point number (e.g. the 5.3 in 5.3e5). The next sections go into more detail on the causes of hardware error on various floating point operations.</p>

<h2>3. Cause of Rounding Error in Division</h2>

<p>The main cause of the error in floating point division is the division algorithms used to calculate the quotient. Most computer systems calculate division using multiplication by an inverse, mainly in <code>Z=X/Y</code>, <code>Z = X * (1/Y)</code>.  A division is computed iteratively i.e. each cycle computes some bits of the quotient until the desired precision is reached, which for IEEE-754 is anything with an error of less than one unit in the last place. The table of reciprocals of Y (1/Y) is known as the quotient selection table (QST) in the slow division, and the size in bits of the quotient selection table is usually the width of the radix, or a number of bits of the quotient computed in each iteration,  plus a few guard bits. For the IEEE-754 standard, double precision (64-bit), it would be the size of the radix of the divider, plus a few guard bits k, where <code>k&gt;=2</code>. So for example, a typical Quotient Selection Table for a divider that computes 2 bits of the quotient at a time (radix 4) would be <code>2+2= 4</code> bits (plus a few optional bits). </p>

<p><strong>3.1 Division Rounding Error: Approximation of Reciprocal</strong></p>

<p>What reciprocals are in the quotient selection table depend on the <a href="http://en.wikipedia.org/wiki/Division_%28digital%29" rel="noreferrer">division method</a>: slow division such as SRT division, or fast division such as Goldschmidt division; each entry is modified according to the division algorithm in an attempt to yield the lowest possible error. In any case, though, all reciprocals are <em>approximations</em> of the actual reciprocal and introduce some element of error. Both slow division and fast division methods calculate the quotient iteratively, i.e. some number of bits of the quotient are calculated each step, then the result is subtracted from the dividend, and the divider repeats the steps until the error is less than one half of one unit in the last place. Slow division methods calculate a fixed number of digits of the quotient in each step and are usually less expensive to build, and fast division methods calculate a variable number of digits per step and are usually more expensive to build. The most important part of the division methods is that most of them rely upon repeated multiplication by an <em>approximation</em> of a reciprocal, so they are prone to error.</p>

<h2>4. Rounding Errors in Other Operations: Truncation</h2>

<p>Another cause of the rounding errors in all operations are the different modes of truncation of the final answer that IEEE-754 allows. There's truncate, round-towards-zero, <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">round-to-nearest (default),</a> round-down, and round-up. All methods introduce an element of error of less than one unit in the last place for a single operation. Over time and repeated operations, truncation also adds cumulatively to the resultant error. This truncation error is especially problematic in exponentiation, which involves some form of repeated multiplication.</p>

<h2>5. Repeated Operations</h2>

<p>Since the hardware that does the floating point calculations only needs to yield a result with an error of less than one half of one unit in the last place for a single operation, the error will grow over repeated operations if not watched. This is the reason that in computations that require a bounded error, mathematicians use methods such as using the round-to-nearest <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">even digit in the last place</a> of IEEE-754, because, over time, the errors are more likely to cancel each other out, and <a href="http://en.wikipedia.org/wiki/Interval_arithmetic" rel="noreferrer">Interval Arithmetic</a> combined with variations of the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules" rel="noreferrer">IEEE 754 rounding modes</a> to predict rounding errors, and correct them. Because of its low relative error compared to other rounding modes, round to nearest even digit (in the last place), is the default rounding mode of IEEE-754.</p>

<p>Note that the default rounding mode, round-to-nearest <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">even digit in the last place</a>, guarantees an error of less than one half of one unit in the last place for one operation. Using the truncation, round-up, and round down alone may result in an error that is greater than one half of one unit in the last place, but less than one unit in the last place, so these modes are not recommended unless they are used in Interval Arithmetic. </p>

<h2>6. Summary</h2>

<p>In short, the fundamental reason for the errors in floating point operations is a combination of the truncation in hardware, and the truncation of a reciprocal in the case of division. Since the IEEE-754 standard only requires an error of less than one half of one unit in the last place for a single operation, the floating point errors over repeated operations will add up unless corrected.</p>
</div><p class="this-has-helped">This answer has helped 621 people.</p><div class="s-prose js-post-body"><p>It's broken in the exact same way the decimal (base-10) notation you learned in grade school and use every day is broken, just for base-2.</p>
<p>To understand, think about representing 1/3 as a decimal value. It's impossible to do exactly! The world will end before you finish writing the 3's after the decimal point, and so instead we write to some number of places and consider it sufficiently accurate.</p>
<p>In the same way, 1/10 (decimal 0.1) cannot be represented exactly in base 2 (binary) as a &quot;decimal&quot; value; a repeating pattern after the decimal point goes on forever. The value is not exact, and therefore you can't do exact math with it using normal floating point methods. Just like with base 10, there are other values that exhibit this problem as well.</p>
</div><p class="this-has-helped">This answer has helped 377 people.</p><div class="s-prose js-post-body"><p><em>Most answers here address this question in very dry, technical terms. I'd like to address this in terms that normal human beings can understand.</em></p>

<p>Imagine that you are trying to slice up pizzas. You have a robotic pizza cutter that can cut pizza slices <em>exactly</em> in half. It can halve a whole pizza, or it can halve an existing slice, but in any case, the halving is always exact.</p>

<p>That pizza cutter has very fine movements, and if you start with a whole pizza, then halve that, and continue halving the smallest slice each time, you can do the halving <em>53 times</em> before the slice is too small for even its high-precision abilities. At that point, you can no longer halve that very thin slice, but must either include or exclude it as is.</p>

<p>Now, how would you piece all the slices in such a way that would add up to one-tenth (0.1) or one-fifth (0.2) of a pizza? Really think about it, and try working it out. You can even try to use a real pizza, if you have a mythical precision pizza cutter at hand. :-)</p>

<hr>

<p>Most experienced programmers, of course, know the real answer, which is that there is no way to piece together an <em>exact</em> tenth or fifth of the pizza using those slices, no matter how finely you slice them. You can do a pretty good approximation, and if you add up the approximation of 0.1 with the approximation of 0.2, you get a pretty good approximation of 0.3, but it's still just that, an approximation.</p>

<p>For double-precision numbers (which is the precision that allows you to halve your pizza 53 times), the numbers immediately less and greater than 0.1 are 0.09999999999999999167332731531132594682276248931884765625 and 0.1000000000000000055511151231257827021181583404541015625. The latter is quite a bit closer to 0.1 than the former, so a numeric parser will, given an input of 0.1, favour the latter.</p>

<p>(The difference between those two numbers is the "smallest slice" that we must decide to either include, which introduces an upward bias, or exclude, which introduces a downward bias. The technical term for that smallest slice is an <a href="https://en.wikipedia.org/wiki/Unit_in_the_last_place">ulp</a>.)</p>

<p>In the case of 0.2, the numbers are all the same, just scaled up by a factor of 2. Again, we favour the value that's slightly higher than 0.2.</p>

<p>Notice that in both cases, the approximations for 0.1 and 0.2 have a slight upward bias. If we add enough of these biases in, they will push the number further and further away from what we want, and in fact, in the case of 0.1 + 0.2, the bias is high enough that the resulting number is no longer the closest number to 0.3.</p>

<p>In particular, 0.1 + 0.2 is really 0.1000000000000000055511151231257827021181583404541015625 + 0.200000000000000011102230246251565404236316680908203125 = 0.3000000000000000444089209850062616169452667236328125, whereas the number closest to 0.3 is actually 0.299999999999999988897769753748434595763683319091796875.</p>

<hr>

<p>P.S. Some programming languages also provide pizza cutters that can <a href="https://en.wikipedia.org/wiki/Decimal_floating_point">split slices into exact tenths</a>. Although such pizza cutters are uncommon, if you do have access to one, you should use it when it's important to be able to get exactly one-tenth or one-fifth of a slice.</p>

<p><a href="http://qr.ae/mDcAq"><em>(Originally posted on Quora.)</em></a></p>
</div><p class="this-has-helped">This answer has helped 229 people.</p><div class="s-prose js-post-body"><p>Floating point rounding errors. 0.1 cannot be represented as accurately in base-2 as in base-10 due to the missing prime factor of 5. Just as 1/3 takes an infinite number of digits to represent in decimal, but is "0.1" in base-3, 0.1 takes an infinite number of digits in base-2 where it does not in base-10. And computers don't have an infinite amount of memory.</p>
</div></body></html>